## Copying file from VM to GCP buckets
gsutil -m cp -r pq/ gs://dtc_data_lake_ny-rides-aldo/pq

## To have spark to connect to gcp
# Download the spark connector to gcp (search gcs connector for hadoop in google)
gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar gcs-connector-hadoop3-2.2.5.jar

## import the necessary libs
import pyspark
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from pyspark.context import SparkContext

credentials_location = '/home/alysis/.gc/xxxxxx.json'

conf = SparkConf() \
    .setMaster('local[*]') \
    .setAppName('test') \
    .set("spark.jars", "../spark/spark-3.2.4-bin-hadoop3.2/jars/gcs-connector-hadoop3-latest.jar") \
    .set("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .set("spark.hadoop.google.cloud.auth.service.account.json.keyfile", credentials_location)

sc = SparkContext(conf=conf)

sc._jsc.hadoopConfiguration().set("fs.AbstractFileSystem.gs.impl",  "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
sc._jsc.hadoopConfiguration().set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
sc._jsc.hadoopConfiguration().set("fs.gs.auth.service.account.json.keyfile", credentials_location)
sc._jsc.hadoopConfiguration().set("fs.gs.auth.service.account.enable", "true")

spark = SparkSession.builder \
    .config(conf=sc.getConf()) \
    .getOrCreate()


## Creating spark cluster on VM
# Docs guide to spark standalone
https://spark.apache.org/docs/latest/spark-standalone.html
# go to spark home
./sbin/start-master.sh
# spark master will open on port 8080
# edit the spark session builder as
spark = SparkSession.builder \
    .master("spark://de-zoomcamp.us-west2-a.c.ny-rides-aldo.internal:7077") \
    .appName('test') \
    .getOrCreate()
# Add a worker
./sbin/start-worker.sh <master-spark-URL>
# Once jupyter nb is ready, convert to script
jupyter nbconvert --to=script 06_spark_sql.ipynb
# Edit the script, and add argparse to allow inputs to the scripts
# Take out the spark master URL, use spark-submit to submit the python job into the spark master
python 06_spark_sql.py \
    --input_green=data/pq/green/2020/*/ \
    --input_yellow=data/pq/yellow/2020/*/ \
    --output=data/report-2020

URL="spark://de-zoomcamp.us-west2-a.c.ny-rides-aldo.internal:7077"

spark-submit \
    --master="${URL}" \
    06_spark_sql.py \
    --input_green=data/pq/green/2020/*/ \
    --input_yellow=data/pq/yellow/2020/*/ \
    --output=data/report-2020

# Stop the spark master and slave
./sbin.stop-slave.sh
./sbin.stop-master.sh