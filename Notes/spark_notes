## spark essential imports to bulid spark sessions
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master("local[*]") \
    .appName('test') \
    .getOrCreate()

## Spark read csv to df
df = spark.read \
     .option("header", "true") \
     .csv('fhvhv_tripdata_2021-01.csv.gz')

## show what the df is
df.show()
df.head(5)
df.schema ## Shows what the data structure is for the df

## Create df from pandas
spark.createDataFrame(df_pandas).show()

## Spark doesnt get iteritems until Spark3.4
df.iteritems = df.items

## Use python to figure out the recommended datatypes form pandas
!head {name of the csv file} > head.csv
# Use this starting point as to edit the remaining data types
# Create pythonized datatype declaration, True indicates if it's nullable
from spark.sql import types
schema = types.StructType([
    types.StructField('hvfhs_license_num', types.StringType(), True),
    types.StructField('dispatching_base_num', types.StringType(), True),
    types.StructField('pickup_datetime', types.TimestampType(), True),
    types.StructField('dropoff_datetime', types.TimestampType(), True),
    types.StructField('PULocationID', types.IntegerType(), True),
    types.StructField('DOLocationID', types.IntegerType(), True),
    types.StructField('SR_Flag', types.StringType(), True)
])
# then using the defined, recall the csv into spark df

## Repartition the spark df to be split into multiple smaller partition
df.partition({insert number of partitions})
# Will not apply until you execute (ex. write as parquet)
df.write.parquet('fhvhv/2021/01') # would not like a if a folder exist, unless added mode='override'

## read back the parquet file
df = spark.read.parquet('fhvhv/2021/01/')

## display the schema of spark df neatly
df.printSchema()

## Select columns and filter
df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \
  .filter(df.hvfhs_license_num == 'HV0003') \
  .show()
# select and filter are lazy and is not executed immediately (transformation), and there is show that is executed immediately (actions)

## Import spark native functions
from pyspark.sql import functions as F

## Add columns
df \
    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \
    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \
    .show()

## Add custom functions
# Define the functions
def crazy_stuff(base_num):
    ....
    return ....
# define it as spark udf
crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())

